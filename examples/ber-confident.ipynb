{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change path, the directory where the source files are located\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "load_model = True\n",
    "model_dir = 'input/ber-sklearn-mlp/'\n",
    "clf_model = 'mlp'\n",
    "data_type = 'processed'\n",
    "data_dir = f'input/ber-processed-parquet/'\n",
    "\n",
    "\n",
    "# Read the processed data from the parquet file\n",
    "processed = pd.read_parquet(f\"{data_dir}/processed.parquet\")\n",
    "\n",
    "# Separate the features (X_processed) and labels (labels)\n",
    "X_processed = processed.drop(columns=[\"EnergyRating\"])\n",
    "labels = processed['EnergyRating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from clear.utils import load_train_test, load_embedding\n",
    "n_splits = 5\n",
    "pred_probs = []\n",
    "embedding_dir = 'input/ber-scarf-train-model/'\n",
    "data_dir = f'input/ber-processed-parquet/'\n",
    "\n",
    "\n",
    "model_dir = 'input/ber-sklearn-models/processed/mlp'\n",
    "clf_model = 'mlp'\n",
    "data_type = 'processed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clear.utils import load_pickle\n",
    "from clear.model_sklearn import get_pred_probs\n",
    "pred_probs_proc = []\n",
    "labels_proc = []\n",
    "for i in range(n_splits):\n",
    "    print(f\"split {i+1}\")\n",
    "    X_train, y_train, X_test, y_test = load_train_test(f'{data_dir}/split_{i+1}', data_type = 'processed', data_format='parquet')\n",
    "    model = load_pickle(f'{model_dir}/split_{i+1}/{clf_model}_{data_type}.pkl')\n",
    "    pred = get_pred_probs(model, X_test)\n",
    "    pred_probs_proc.append(pred)\n",
    "    labels_proc.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clear.confident_learning import find_merged_classes\n",
    "from clear.utils import get_ber_class_mapping\n",
    "\n",
    "class_names, class_mapping = get_ber_class_mapping()\n",
    "overlapped, merged, merged_dict = find_merged_classes(\n",
    "    labels_proc[0], class_names, pred_probs_proc[0], thresh=0.02\n",
    ")\n",
    "merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.DataFrame.from_dict(merged_dict, orient='index')\n",
    "df_merged = df_merged.reset_index()\n",
    "df_merged.columns = ['origin', 'merged']\n",
    "output_dir = 'output/'\n",
    "df_merged.to_csv(f'{output_dir}/merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clear.confident_learning import merge_label\n",
    "from clear.utils import get_scores, save_model_pickle\n",
    "from clear.model_sklearn import get_clf_mlp, get_clf_rf\n",
    "\n",
    "compute_processed = True\n",
    "model_type = 'rf'\n",
    "\n",
    "for i in range(n_splits):\n",
    "    X_train, y_train, X_test, y_test = load_train_test(f'{data_dir}/split_{i+1}', data_type = 'processed', data_format='parquet')\n",
    "    y_train_merged = merge_label(y_train, merged_dict)\n",
    "    y_test_merged = merge_label(y_test, merged_dict)\n",
    "\n",
    "    #processed\n",
    "    if compute_processed:\n",
    "        #use the following to load a pretrained model\n",
    "        # model = load_pickle(f'{model_dir}/split_{i+1}/{clf_model}_{data_type}.pkl')\n",
    "        #use sklearn model from scratch (mlp pytorch model is implemented under the pytorch folder)\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            clf, clf_model = get_clf_mlp()\n",
    "        elif model_type == 'rf':\n",
    "            clf, clf_model = get_clf_rf()\n",
    "        else:\n",
    "            raise('Model type not supported')\n",
    "\n",
    "        clf.fit(X_train, y_train_merged)\n",
    "        save_model_pickle(output_dir, clf, 'processed', clf_model)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        scores = get_scores(y_test_merged, y_pred)\n",
    "        scores_emb = pd.DataFrame.from_dict(scores)\n",
    "        scores_emb.to_csv(f'{output_dir}/scores_processed_{i+1}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_scores_mean(folder, fname, n_splits=5):\n",
    "    scores = []\n",
    "    for i in range(n_splits):\n",
    "        curr = pd.read_csv(f'{folder}/{fname}_{i+1}.csv')\n",
    "        scores.append(curr)\n",
    "\n",
    "    scores = pd.concat(scores)\n",
    "    return scores.mean(axis=0)\n",
    "\n",
    "avg_scores = get_scores_mean(output_dir, 'scores_processed')\n",
    "avg_scores.to_csv(f'{output_dir}/avg_scores.csv', index=False)\n",
    "avg_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ber_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
