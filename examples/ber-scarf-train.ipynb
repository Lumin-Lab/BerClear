{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#change path, the directory where the source files are located\n","import os\n","os.chdir(\"..\")\n","import sys\n","sys.path.append(\"clear/pytorch\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:19.264639Z","iopub.status.busy":"2024-11-02T16:46:19.263869Z","iopub.status.idle":"2024-11-02T16:46:19.523393Z","shell.execute_reply":"2024-11-02T16:46:19.521872Z","shell.execute_reply.started":"2024-11-02T16:46:19.264552Z"},"trusted":true},"outputs":[],"source":["wandb_key = \"YOUR_WANDB_KEY\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:19.526908Z","iopub.status.busy":"2024-11-02T16:46:19.526348Z","iopub.status.idle":"2024-11-02T16:46:23.120766Z","shell.execute_reply":"2024-11-02T16:46:23.119485Z","shell.execute_reply.started":"2024-11-02T16:46:19.526833Z"},"trusted":true},"outputs":[],"source":["import wandb\n","wandb.login(key=wandb_key)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:23.122988Z","iopub.status.busy":"2024-11-02T16:46:23.122299Z","iopub.status.idle":"2024-11-02T16:46:23.518178Z","shell.execute_reply":"2024-11-02T16:46:23.517110Z","shell.execute_reply.started":"2024-11-02T16:46:23.122930Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","data_folder = 'ber-processed-parquet'\n","input_dir = f\"input/{data_folder}\"\n","output_dir = \"output\"\n","config_dir=f\"clear/pytorch/configs\"\n","scarf_model_name = \"scarf\"\n","mlp_model_name = \"mlp\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:23.520605Z","iopub.status.busy":"2024-11-02T16:46:23.519839Z","iopub.status.idle":"2024-11-02T16:46:23.557921Z","shell.execute_reply":"2024-11-02T16:46:23.556623Z","shell.execute_reply.started":"2024-11-02T16:46:23.520547Z"},"trusted":true},"outputs":[],"source":["import os\n","n_splits = len(list(os.walk(input_dir))[0][1])\n","n_splits"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-02T16:46:23.562095Z","iopub.status.busy":"2024-11-02T16:46:23.561576Z","iopub.status.idle":"2024-11-02T16:46:23.568224Z","shell.execute_reply":"2024-11-02T16:46:23.567050Z","shell.execute_reply.started":"2024-11-02T16:46:23.562040Z"},"trusted":true},"outputs":[],"source":["scarf_batch_size = 32\n","scarf_epochs = 10\n","scarf_lr = 0.001\n","scarf_emb_dim = 32\n","scarf_encoder_depth = 3\n","scarf_corruption_rate=0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:23.570017Z","iopub.status.busy":"2024-11-02T16:46:23.569648Z","iopub.status.idle":"2024-11-02T16:46:23.579456Z","shell.execute_reply":"2024-11-02T16:46:23.578230Z","shell.execute_reply.started":"2024-11-02T16:46:23.569976Z"},"trusted":true},"outputs":[],"source":["splits = range(n_splits)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:23.594923Z","iopub.status.busy":"2024-11-02T16:46:23.594465Z","iopub.status.idle":"2024-11-02T16:46:27.795579Z","shell.execute_reply":"2024-11-02T16:46:27.794390Z","shell.execute_reply.started":"2024-11-02T16:46:23.594871Z"},"trusted":true},"outputs":[],"source":["import clear.pytorch.src as src\n","from src.utils import set_seed, load_from_yaml, get_features_from_yaml\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:27.797989Z","iopub.status.busy":"2024-11-02T16:46:27.797258Z","iopub.status.idle":"2024-11-02T16:46:27.803916Z","shell.execute_reply":"2024-11-02T16:46:27.802874Z","shell.execute_reply.started":"2024-11-02T16:46:27.797931Z"},"trusted":true},"outputs":[],"source":["flag_train = False\n","\n","flag_embedding = True\n","\n","mode = 'full'\n","# mode = 'batch'\n","\n","model_dir = 'output'\n","output_dir = 'output'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:27.805715Z","iopub.status.busy":"2024-11-02T16:46:27.805300Z","iopub.status.idle":"2024-11-02T16:46:27.892412Z","shell.execute_reply":"2024-11-02T16:46:27.891172Z","shell.execute_reply.started":"2024-11-02T16:46:27.805667Z"},"trusted":true},"outputs":[],"source":["import argparse\n","import os\n","import pandas as pd\n","import wandb\n","from src.utils import set_seed, load_from_yaml, get_features_from_yaml\n","from src.data_processor import DataProcessor\n","from src.scarf import SCARF\n","from src.dataloader import ScarfToDataLoader\n","from train import train_encoder\n","from dotenv import load_dotenv\n","# Suppress Dtype and Future warnings from pandas\n","import warnings\n","warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","import torch\n","# Set a consistent seed for reproducibility\n","set_seed(42)\n","load_dotenv()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:27.894570Z","iopub.status.busy":"2024-11-02T16:46:27.894146Z","iopub.status.idle":"2024-11-02T16:46:27.918240Z","shell.execute_reply":"2024-11-02T16:46:27.916819Z","shell.execute_reply.started":"2024-11-02T16:46:27.894527Z"},"trusted":true},"outputs":[],"source":["\n","def run_scarf(arguments):\n","    # Argument parsing\n","    parser = argparse.ArgumentParser(description='Train SCARF model')\n","    parser.add_argument('--config_dir', default='configs', help='Directory for configuration files')\n","    parser.add_argument('--output_dir', default='exp', help='Output directory for models and stats')\n","    parser.add_argument('--train_data_path', default='data/small_train.csv', help='Data directory')\n","    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n","    parser.add_argument('--epochs', type=int, default=1, help='Number of training epochs')\n","    parser.add_argument('--lr', type=float, default=3e-5, help='Learning rate')\n","    parser.add_argument('--emb_dim', type=int, default=32, help='Dimensionality of the embedding space')\n","    parser.add_argument('--encoder_depth', type=int, default=3, help='Depth of the encoder model')\n","    parser.add_argument('--model_name', type=str, default=\"scarf\", help='Name of saved model')\n","    parser.add_argument('--corruption_rate', type=float, default=0.3, help='Rate of corruption applied during training')\n","    # parser.add_argument('--device', type=str, default=\"cpu\")\n","    parser.add_argument('--wandb_project_name', type=str, required=True, help='Name of wandb project')\n","    parser.add_argument('--wandb_entity', type=str, default=\"urbancomp\", help='Name of wandb entity')\n","    parser.add_argument('--wandb_key', type=str)\n","    args = parser.parse_args(arguments)\n","\n","    # Ensure output directory exists\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    # Load configurations from YAML files\n","    preprocessing_config = load_from_yaml(f\"{args.config_dir}/preprocess_config.yaml\")\n","    energy_config = load_from_yaml(f\"{args.config_dir}/energy_config.yaml\")\n","    column_type_path = f\"{args.output_dir}/column_type_classification.yaml\"\n","    train_stats_path = f\"{args.output_dir}/train_stats.json\"\n","    scaler_path = f\"{args.output_dir}/scaler.joblib\"\n","    encoder_path = f\"{args.output_dir}/encoder.joblib\"\n","    small_area_path = f\"{args.config_dir}/small_area.yaml\"\n","    target = \"EnergyRating\"\n","    feature_config = load_from_yaml(f\"{args.config_dir}/column_type_classification.yaml\")\n","    features = get_features_from_yaml(feature_config, target)\n","    energyRatingEncoding = energy_config[\"original_rating_encoding\"]\n","\n","\n","    wandb_key = os.getenv('WANDB_KEY')\n","    if not wandb_key:\n","        wandb_key = args.wandb_key\n","\n","    # Initialize Weights & Biases\n","    wandb.login(key=wandb_key)\n","    wandb.init(\n","        project=\"Scarf\",\n","        name=args.wandb_project_name,\n","        entity=args.wandb_entity,\n","        config={\n","            \"epochs\": args.epochs,\n","            \"batch_size\": args.batch_size,\n","            \"lr\": args.lr,\n","            \"feature_num\": len(features),\n","            \"class_num\": len(energyRatingEncoding),\n","            \"features\": features,\n","            \"model_save_dir\": args.output_dir,\n","            \"model_name\": args.model_name,\n","            \"emb_dim\": args.emb_dim,\n","            \"encoder_depth\": args.encoder_depth,\n","            \"corruption_rate\": args.corruption_rate,\n","        }\n","    )\n","\n","    # Load datasets\n","    data_format = args.train_data_path.split('.')[-1]\n","    if data_format == 'csv':\n","        df_train = pd.read_csv(f\"{args.train_data_path}\")\n","    elif data_format == 'parquet':\n","        df_train = pd.read_parquet(f\"{args.train_data_path}\")\n","    else:\n","        print(\"wrong data format\")\n","        return\n","\n","\n","    # # if not processed, Process datasets\n","    # processor = DataProcessor(preprocessing_config, train_stats_path, column_type_path, scaler_path, encoder_path, small_area_path, target, features)\n","    # train_df = processor.process(df_train, is_train=True)\n","\n","\n","    device = \"cuda\" if  torch.cuda.is_available() else \"cpu\"\n","    # Initialize and train the model\n","    model = SCARF(input_dim=df_train.shape[1]-1, emb_dim=args.emb_dim, encoder_depth=args.encoder_depth, corruption_rate=args.corruption_rate)\n","    model.to(device)\n","    train_encoder(df_train, \n","                  ScarfToDataLoader, \n","                  model, \n","                  device=device, \n","                  target_col=target, \n","                  batch_size=args.batch_size, \n","                  lr=args.lr, \n","                  epochs=args.epochs, \n","                  model_save_dir=args.output_dir, \n","                  model_name=args.model_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:27.938884Z","iopub.status.busy":"2024-11-02T16:46:27.938500Z","iopub.status.idle":"2024-11-02T16:46:27.950758Z","shell.execute_reply":"2024-11-02T16:46:27.949481Z","shell.execute_reply.started":"2024-11-02T16:46:27.938830Z"},"trusted":true},"outputs":[],"source":["import os\n","\"\"\"The trained scarf model is saved in \n","{root_path}/output/split_{split}/scarf.pt if you run the following command:\n","\"\"\"\n","def train_scarf(splits=5, batch=True, \n","                tag='train'):\n","    arguments = [\n","          f'--config_dir={config_dir}',\n","          f\"--batch_size={scarf_batch_size}\",\n","          f\"--epochs={scarf_epochs}\",\n","          f\"--lr={scarf_lr}\",\n","          f\"--emb_dim={scarf_emb_dim}\",\n","          f\"--encoder_depth={scarf_encoder_depth}\",\n","          f\"--model_name={scarf_model_name}\",\n","          f\"--corruption_rate={scarf_corruption_rate}\",\n","          f\"--wandb_project_name=SCARF_Project\",\n","          f\"--wandb_entity=urbancomp\",\n","        ]\n","    if batch == True:\n","        for i in splits:\n","            args = arguments + [\n","              f'--output_dir={output_dir}/split_{i+1}',\n","              f'--train_data_path={input_dir}/split_{i+1}/processed_{tag}.parquet',\n","            ]\n","            run_scarf(args)\n","    else:\n","        args = arguments + [\n","            f'--output_dir={output_dir}/',\n","            f'--train_data_path={input_dir}/processed.parquet',\n","        ]\n","        run_scarf(args)\n","\n","flag_train = True\n","if flag_train:\n","    if mode == 'batch':\n","        train_scarf(batch=True)\n","    if mode == 'full':\n","        train_scarf(batch=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:27.955895Z","iopub.status.busy":"2024-11-02T16:46:27.955404Z","iopub.status.idle":"2024-11-02T16:46:27.983248Z","shell.execute_reply":"2024-11-02T16:46:27.981959Z","shell.execute_reply.started":"2024-11-02T16:46:27.955830Z"},"trusted":true},"outputs":[],"source":["import argparse\n","import os\n","import pandas as pd\n","import torch\n","from src.utils import set_seed, load_from_yaml, get_features_from_yaml, load_model\n","from src.data_processor import DataProcessor\n","from src.scarf import SCARF\n","from src.dataloader import ScarfToDataLoader\n","# Suppress Dtype and Future warnings from pandas\n","import warnings\n","import numpy as np\n","\n","\n","\n","warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","\n","# Set a consistent seed for reproducibility\n","set_seed(42)\n","\n","def get_scarf_embedding(arguments):\n","    # Argument parsing\n","    parser = argparse.ArgumentParser(description='Generate Embeddings')\n","    parser.add_argument('--config_dir', default='configs', help='Directory for configuration files')\n","    parser.add_argument('--model_dir', default='exp', help='Input Model directory for models')\n","    parser.add_argument('--output_dir', default='exp', help='Output directory for models and stats')\n","    parser.add_argument('--data_path', type=str, help='Specify the data path for the file you want to convert into SCARF embeddings.')\n","    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n","    parser.add_argument('--epochs', type=int, default=1, help='Number of training epochs')\n","    parser.add_argument('--lr', type=float, default=3e-5, help='Learning rate')\n","    parser.add_argument('--emb_dim', type=int, default=32, help='Dimensionality of the embedding space')\n","    parser.add_argument('--encoder_depth', type=int, default=3, help='Depth of the encoder model')\n","    parser.add_argument('--model_name', type=str, default=\"scarf\", help='Name of saved model')\n","    parser.add_argument('--corruption_rate', type=float, default=0.3, help='Rate of corruption applied during training')\n","    # parser.add_argument('--device', type=str, default=\"cpu\")\n","    parser.add_argument('--embedding_save_name', type=str, required=True)\n","    args = parser.parse_args(arguments)\n","\n","    # Ensure output directory exists\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    device = \"cuda\" if  torch.cuda.is_available() else \"cpu\"\n","\n","    # Load configurations from YAML files\n","    preprocessing_config = load_from_yaml(f\"{args.config_dir}/preprocess_config.yaml\")\n","    energy_config = load_from_yaml(f\"{args.config_dir}/energy_config.yaml\")\n","    column_type_path = f\"{args.output_dir}/column_type_classification.yaml\"\n","    train_stats_path = f\"{args.output_dir}/train_stats.json\"\n","    scaler_path = f\"{args.output_dir}/scaler.joblib\"\n","    encoder_path = f\"{args.output_dir}/encoder.joblib\"\n","    small_area_path = f\"{args.config_dir}/small_area.yaml\"\n","    target = \"EnergyRating\"\n","    feature_config = load_from_yaml(f\"{args.config_dir}/column_type_classification.yaml\")\n","    features = get_features_from_yaml(feature_config, target)\n","    energyRatingEncoding = energy_config[\"original_rating_encoding\"]\n","\n","\n","    # Load datasets\n","    data_format = args.data_path.split('.')[-1]\n","    if data_format == 'csv':\n","        df = pd.read_csv(f\"{args.data_path}\")\n","    elif data_format == 'parquet':\n","        df = pd.read_parquet(f\"{args.data_path}\")\n","    else:\n","        print(\"wrong data format\")\n","        return\n","\n","    # Process datasets\n","    # processor = DataProcessor(preprocessing_config, train_stats_path, column_type_path, scaler_path, encoder_path, small_area_path, target, features)\n","    # data_df = processor.process(df, is_train=True)\n","\n","    # Initialize and train the model\n","    model = SCARF(input_dim=df.shape[1]-1, emb_dim=args.emb_dim, encoder_depth=args.encoder_depth, corruption_rate=args.corruption_rate)\n","    model = load_model(model_dir = args.model_dir, \n","               model_name= args.model_name,\n","               model = model,\n","               device = device)\n","\n","    model.eval()\n","    embeddings = []\n","    dataloader = ScarfToDataLoader(df, \n","                                    target_col=target, \n","                                    batch_size=args.batch_size, \n","                                    shuffle=False).dataloader\n","\n","    with torch.no_grad():\n","        for f, _ in dataloader:\n","            embeddings.append(model.get_embeddings(f.to(device)))\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    embeddings = embeddings.cpu()\n","    embeddings_numpy= embeddings.numpy()\n","    np.save(f\"{args.output_dir}/{args.embedding_save_name}.npy\", embeddings_numpy)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:46:27.984820Z","iopub.status.busy":"2024-11-02T16:46:27.984445Z","iopub.status.idle":"2024-11-02T16:47:09.867312Z","shell.execute_reply":"2024-11-02T16:47:09.866126Z","shell.execute_reply.started":"2024-11-02T16:46:27.984779Z"},"trusted":true},"outputs":[],"source":["\"\"\"The generated embeddings are saved as a NumPy array in \n","output/split_{split}/train.npy for batch mode, \n","or\n","output/processed.npy for full mode:\n","\"\"\"\n","def get_embeddings(output_dir, model_dir, splits=5, batch=True, tag='train'):\n","    arguments = [\n","          f\"--config_dir={config_dir}\",\n","\n","          f\"--batch_size={scarf_batch_size}\",\n","          f\"--epochs={scarf_epochs}\",\n","          f\"--lr={scarf_lr}\",\n","          f\"--emb_dim={scarf_emb_dim}\",\n","          f\"--encoder_depth={scarf_encoder_depth}\",\n","          f\"--model_name={scarf_model_name}\",\n","          f\"--corruption_rate={scarf_corruption_rate}\",\n","          f\"--embedding_save_name={tag}\",\n","        ]\n","    if batch == True:\n","        for i in splits:\n","            args = arguments + [\n","          f\"--output_dir={output_dir}/split_{i+1}\",\n","          f\"--model_dir={model_dir}/split_{i+1}\",\n","          f\"--data_path={input_dir}/split_{i+1}/processed_{tag}.parquet\",\n","            ]\n","            get_scarf_embedding(args)\n","    else:\n","        args = arguments + [\n","          f\"--output_dir={output_dir}/\",\n","          f\"--model_dir={model_dir}/\",\n","          f\"--data_path={input_dir}/processed.parquet\",\n","        ]\n","        get_scarf_embedding(args)\n","\n","\n","if flag_embedding:\n","    if mode == 'batch':\n","        get_embeddings(output_dir, \n","                       model_dir, splits=5)\n","    if mode == 'full':\n","        get_embeddings(output_dir, \n","                       model_dir, batch=False, tag='processed')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-02T16:47:11.113196Z","iopub.status.busy":"2024-11-02T16:47:11.112761Z","iopub.status.idle":"2024-11-02T16:47:12.289006Z","shell.execute_reply":"2024-11-02T16:47:12.287590Z","shell.execute_reply.started":"2024-11-02T16:47:11.113137Z"},"trusted":true},"outputs":[],"source":["!ls output"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5621819,"sourceId":9286959,"sourceType":"datasetVersion"},{"datasetId":5625724,"sourceId":9293454,"sourceType":"datasetVersion"},{"datasetId":5630752,"sourceId":9299748,"sourceType":"datasetVersion"},{"datasetId":5963657,"sourceId":9788133,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
